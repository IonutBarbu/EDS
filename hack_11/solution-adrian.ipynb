{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_signals(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        data = fp.read().splitlines()\n",
    "        data = map(lambda x: x.rstrip().lstrip().split(), data)\n",
    "        data = [list(map(float, line)) for line in data]\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_labels(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        activities = fp.read().splitlines()\n",
    "        activities = list(map(int, activities))\n",
    "    return np.array(activities)\n",
    "\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "\n",
    "def one_hot_encode(np_array, num_labels):\n",
    "    return (np.arange(num_labels) == np_array[:, None]).astype(np.float32)\n",
    "\n",
    "\n",
    "def reformat_data(dataset, labels):\n",
    "    no_labels = len(np.unique(labels))\n",
    "    labels = one_hot_encode(labels, no_labels)\n",
    "    dataset, labels = randomize(dataset, labels)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "d_activity_num_to_labels = {\n",
    "    1: 'walking',\n",
    "    2: 'walking upstairs',\n",
    "    3: 'walking downstairs',\n",
    "    4: 'sitting',\n",
    "    5: 'standing',\n",
    "    6: 'laying'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset contains 7352 signals, each one of length 128 and 9 components \n",
      "The test dataset contains 2947 signals, each one of length 128 and 9 components \n",
      "The train dataset contains 7352 labels, with the following distribution:\n",
      " Counter({6: 1407, 5: 1374, 4: 1286, 1: 1226, 2: 1073, 3: 986})\n",
      "The test dataset contains 2947 labels, with the following distribution:\n",
      " Counter({6: 537, 5: 532, 1: 496, 4: 491, 2: 471, 3: 420})\n"
     ]
    }
   ],
   "source": [
    "INPUT_FOLDER_TRAIN = 'UCI HAR Dataset/train/Inertial Signals/'\n",
    "INPUT_FOLDER_TEST = 'UCI HAR Dataset/test/Inertial Signals/'\n",
    "\n",
    "INPUT_FILES_TRAIN = [\n",
    "    'body_acc_x_train.txt', 'body_acc_y_train.txt', 'body_acc_z_train.txt',\n",
    "    'body_gyro_x_train.txt', 'body_gyro_y_train.txt', 'body_gyro_z_train.txt',\n",
    "    'total_acc_x_train.txt', 'total_acc_y_train.txt', 'total_acc_z_train.txt'\n",
    "]\n",
    "\n",
    "INPUT_FILES_TEST = [\n",
    "    'body_acc_x_test.txt', 'body_acc_y_test.txt', 'body_acc_z_test.txt',\n",
    "    'body_gyro_x_test.txt', 'body_gyro_y_test.txt', 'body_gyro_z_test.txt',\n",
    "    'total_acc_x_test.txt', 'total_acc_y_test.txt', 'total_acc_z_test.txt'\n",
    "]\n",
    "\n",
    "LABELFILE_TRAIN = 'UCI HAR Dataset/train/y_train.txt'\n",
    "LABELFILE_TEST = 'UCI HAR Dataset/test/y_test.txt'\n",
    "\n",
    "train_signals, test_signals = [], []\n",
    "\n",
    "for input_file in INPUT_FILES_TRAIN:\n",
    "    signal = read_signals(INPUT_FOLDER_TRAIN + input_file)\n",
    "    train_signals.append(signal)\n",
    "train_signals = np.transpose(np.array(train_signals), (1, 2, 0))\n",
    "\n",
    "for input_file in INPUT_FILES_TEST:\n",
    "    signal = read_signals(INPUT_FOLDER_TEST + input_file)\n",
    "    test_signals.append(signal)\n",
    "test_signals = np.transpose(np.array(test_signals), (1, 2, 0))\n",
    "\n",
    "train_labels = read_labels(LABELFILE_TRAIN)\n",
    "test_labels = read_labels(LABELFILE_TEST)\n",
    "\n",
    "[no_signals_train, no_steps_train,\n",
    " no_components_train] = np.shape(train_signals)\n",
    "[no_signals_test, no_steps_test, no_components_test] = np.shape(test_signals)\n",
    "no_labels = len(np.unique(train_labels[:]))\n",
    "\n",
    "print(\n",
    "    \"The train dataset contains {} signals, each one of length {} and {} components \".\n",
    "    format(no_signals_train, no_steps_train, no_components_train))\n",
    "print(\n",
    "    \"The test dataset contains {} signals, each one of length {} and {} components \".\n",
    "    format(no_signals_test, no_steps_test, no_components_test))\n",
    "print(\n",
    "    \"The train dataset contains {} labels, with the following distribution:\\n {}\".\n",
    "    format(np.shape(train_labels)[0], Counter(train_labels[:])))\n",
    "print(\n",
    "    \"The test dataset contains {} labels, with the following distribution:\\n {}\".\n",
    "    format(np.shape(test_labels)[0], Counter(test_labels[:])))\n",
    "\n",
    "train_data, train_labels = reformat_data(train_signals, train_labels)\n",
    "test_data, test_labels = reformat_data(test_signals, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below my solution. Changes:\n",
    "- model constructed inline, removed the original fccd model ... (just preference)\n",
    "+ **multi-layer RNN**\n",
    "+ **dropout**\n",
    "+ calculating everything in tensorflow (including accuracy)\n",
    "+ **tensorboard** logging, last point is run on the test set (nice for comparing runs)\n",
    "+ testing just once after training on the full test set\n",
    "+ tuned hyperparameters, for test loss above **70%**\n",
    "- model still overfits (;/)\n",
    "\n",
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset \n",
    "NUM_STEPS = 128\n",
    "NUM_COMPONENTS = 9\n",
    "NUM_LABLES = 6\n",
    "\n",
    "# network architecture\n",
    "NUM_UNITS = 256\n",
    "NUM_LAYERS = 3\n",
    "DROPOUT = True\n",
    "\n",
    "# training\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.00005\n",
    "TOTAL_STEPS = 5000\n",
    "LOG_STEP = 100\n",
    "DISPLAY_STEP = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # First make placeholders for data.\n",
    "    # Use None for batch size, to be more flexible.\n",
    "    # That will allow for testing on full test set\n",
    "    tf_dataset = tf.placeholder(tf.float32, shape=[None, NUM_STEPS, NUM_COMPONENTS])\n",
    "    tf_labels = tf.placeholder(tf.float32, shape=[None, NUM_LABLES])\n",
    "\n",
    "    # Build the model\n",
    "    data = tf.unstack(tf_dataset, axis=1)\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(NUM_UNITS)\n",
    "    if DROPOUT:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, 0.25)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [tf.contrib.rnn.BasicLSTMCell(NUM_UNITS) for _ in range(NUM_LAYERS)])\n",
    "\n",
    "    output, state = tf.contrib.rnn.static_rnn(cell, data, dtype=tf.float32)\n",
    "\n",
    "    logits = tf.layers.dense(output[-1], 6)\n",
    "\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    correct_prediction = tf.equal(\n",
    "        tf.argmax(logits, 1), tf.argmax(tf_labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=tf_labels))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    summ = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct the tensorboard run name\n",
    "now = datetime.datetime.now()\n",
    "LOGDIR = (\"/tmp/hack_11_sec/\" +\n",
    "          '{:02}:{:02}'.format(now.hour, now.minute) +\n",
    "          '_bs{}'.format(BATCH_SIZE) + \n",
    "          '_un{}'.format(NUM_UNITS) + \n",
    "          '_la{}'.format(NUM_LAYERS) + \n",
    "          '_lr{}'.format(LEARNING_RATE))\n",
    "if DROPOUT: LOGDIR += '_d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with learning rate of  5e-05\n",
      "step    0 : loss is   1.52, accuracy on training set   0.00 %\n",
      "step  100 : loss is   0.95, accuracy on training set  40.62 %\n",
      "step  200 : loss is   0.91, accuracy on training set  43.75 %\n",
      "step  300 : loss is   0.88, accuracy on training set  28.12 %\n",
      "step  400 : loss is   0.78, accuracy on training set  50.00 %\n",
      "step  500 : loss is   0.65, accuracy on training set  68.75 %\n",
      "step  600 : loss is   0.77, accuracy on training set  43.75 %\n",
      "step  700 : loss is   0.59, accuracy on training set  62.50 %\n",
      "step  800 : loss is   0.54, accuracy on training set  53.12 %\n",
      "step  900 : loss is   0.36, accuracy on training set  56.25 %\n",
      "step 1000 : loss is   0.68, accuracy on training set  56.25 %\n",
      "step 1100 : loss is   0.56, accuracy on training set  59.38 %\n",
      "step 1200 : loss is   0.42, accuracy on training set  50.00 %\n",
      "step 1300 : loss is   0.40, accuracy on training set  62.50 %\n",
      "step 1400 : loss is   0.33, accuracy on training set  75.00 %\n",
      "step 1500 : loss is   0.33, accuracy on training set  84.38 %\n",
      "step 1600 : loss is   0.44, accuracy on training set  68.75 %\n",
      "step 1700 : loss is   0.11, accuracy on training set  81.25 %\n",
      "step 1800 : loss is   0.48, accuracy on training set  56.25 %\n",
      "step 1900 : loss is   0.25, accuracy on training set  81.25 %\n",
      "step 2000 : loss is   0.13, accuracy on training set  71.88 %\n",
      "step 2100 : loss is   0.46, accuracy on training set  65.62 %\n",
      "step 2200 : loss is   0.05, accuracy on training set  75.00 %\n",
      "step 2300 : loss is   0.04, accuracy on training set  75.00 %\n",
      "step 2400 : loss is   0.19, accuracy on training set  71.88 %\n",
      "step 2500 : loss is   0.14, accuracy on training set  78.12 %\n",
      "step 2600 : loss is   0.13, accuracy on training set  71.88 %\n",
      "step 2700 : loss is   0.04, accuracy on training set  90.62 %\n",
      "step 2800 : loss is   0.10, accuracy on training set  81.25 %\n",
      "step 2900 : loss is   0.22, accuracy on training set  75.00 %\n",
      "step 3000 : loss is   0.06, accuracy on training set  81.25 %\n",
      "step 3100 : loss is   0.10, accuracy on training set  68.75 %\n",
      "step 3200 : loss is   0.13, accuracy on training set  84.38 %\n",
      "step 3300 : loss is   0.05, accuracy on training set  84.38 %\n",
      "step 3400 : loss is   0.08, accuracy on training set  81.25 %\n",
      "step 3500 : loss is   0.15, accuracy on training set  81.25 %\n",
      "step 3600 : loss is   0.06, accuracy on training set  71.88 %\n",
      "step 3700 : loss is   0.17, accuracy on training set  75.00 %\n",
      "step 3800 : loss is   0.05, accuracy on training set  75.00 %\n",
      "step 3900 : loss is   0.11, accuracy on training set  75.00 %\n",
      "step 4000 : loss is   0.06, accuracy on training set  90.62 %\n",
      "step 4100 : loss is   0.10, accuracy on training set  84.38 %\n",
      "step 4200 : loss is   0.22, accuracy on training set  59.38 %\n",
      "step 4300 : loss is   0.15, accuracy on training set  84.38 %\n",
      "step 4400 : loss is   0.14, accuracy on training set  81.25 %\n",
      "step 4500 : loss is   0.03, accuracy on training set  78.12 %\n",
      "step 4600 : loss is   0.08, accuracy on training set  65.62 %\n",
      "step 4700 : loss is   0.06, accuracy on training set  62.50 %\n",
      "step 4800 : loss is   0.24, accuracy on training set  65.62 %\n",
      "step 4900 : loss is   0.27, accuracy on training set  84.38 %\n",
      "Test loss is   0.36, accuracy on training set  71.36 %\n"
     ]
    }
   ],
   "source": [
    "# train and test the model\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized with learning rate of ', LEARNING_RATE)\n",
    "\n",
    "    writer = tf.summary.FileWriter(LOGDIR)\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    for step in range(TOTAL_STEPS):\n",
    "\n",
    "        offset = (step * BATCH_SIZE) % (train_labels.shape[0] - BATCH_SIZE)\n",
    "        batch_data = train_data[offset:(offset + BATCH_SIZE), :, :]\n",
    "        batch_labels = train_labels[offset:(offset + BATCH_SIZE), :]\n",
    "\n",
    "        # train the weights\n",
    "        feed_dict = {tf_dataset: batch_data, tf_labels: batch_labels}\n",
    "        session.run([optimizer], feed_dict)\n",
    "\n",
    "        # log for tensorboard\n",
    "        if step % LOG_STEP == 0:\n",
    "            _, s = session.run([loss, summ], feed_dict)\n",
    "            writer.add_summary(s, step)\n",
    "            writer.flush()\n",
    "\n",
    "        # print during training\n",
    "        if step % DISPLAY_STEP == 0:\n",
    "            l, train_accuracy = session.run([loss, accuracy], feed_dict)\n",
    "            message = \"step {:4} : loss is {:6.2f}, accuracy on training set {:6.2f} %\".format(\n",
    "                step, l, train_accuracy * 100)\n",
    "            print(message)\n",
    "\n",
    "    # calculate and print the accuract on the full test set\n",
    "    feed_dict = {tf_dataset: test_data, tf_labels: test_labels}\n",
    "    l, test_accuracy, s = session.run([loss, accuracy, summ], feed_dict)\n",
    "    writer.add_summary(s, step)\n",
    "    writer.flush()\n",
    "    message = \"Test loss is {:6.2f}, accuracy on training set {:6.2f} %\".format(\n",
    "        l, test_accuracy * 100)\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "99px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
